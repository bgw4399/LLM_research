{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c96ec94c-7712-4cef-9301-9821cfb2f472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:41:43.358829Z",
     "iopub.status.busy": "2024-03-24T11:41:43.358514Z",
     "iopub.status.idle": "2024-03-24T11:42:20.398229Z",
     "shell.execute_reply": "2024-03-24T11:42:20.396372Z",
     "shell.execute_reply.started": "2024-03-24T11:41:43.358795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/20223149/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()\n",
    "\n",
    "#hugging face token 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e88e86-3ce3-43e1-9818-7ee627b987b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:42:31.965271Z",
     "iopub.status.busy": "2024-03-24T11:42:31.964554Z",
     "iopub.status.idle": "2024-03-24T11:42:37.880994Z",
     "shell.execute_reply": "2024-03-24T11:42:37.879307Z",
     "shell.execute_reply.started": "2024-03-24T11:42:31.965200Z"
    }
   },
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"beomi/KoAlpaca-v1.1a\"\n",
    "#data set load\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297ae502-e14c-4c51-bf9b-324bdd23f02d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:42:47.373465Z",
     "iopub.status.busy": "2024-03-24T11:42:47.372635Z",
     "iopub.status.idle": "2024-03-24T11:42:47.443291Z",
     "shell.execute_reply": "2024-03-24T11:42:47.441716Z",
     "shell.execute_reply.started": "2024-03-24T11:42:47.373374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?',\n",
       " 'output': '양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.',\n",
       " 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=55320268'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a034a65-7451-4f8e-9df7-bf909fb25e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:42:49.443654Z",
     "iopub.status.busy": "2024-03-24T11:42:49.442930Z",
     "iopub.status.idle": "2024-03-24T11:42:49.453598Z",
     "shell.execute_reply": "2024-03-24T11:42:49.452088Z",
     "shell.execute_reply.started": "2024-03-24T11:42:49.443590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output', 'url'],\n",
       "        num_rows: 21155\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c99eef14-32a0-47e3-af75-dee37503a3e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:42:51.773755Z",
     "iopub.status.busy": "2024-03-24T11:42:51.772847Z",
     "iopub.status.idle": "2024-03-24T11:42:51.786275Z",
     "shell.execute_reply": "2024-03-24T11:42:51.784688Z",
     "shell.execute_reply.started": "2024-03-24T11:42:51.773688Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21155"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe87f59-793c-4ed0-bace-033b9f8e1419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:42:53.488339Z",
     "iopub.status.busy": "2024-03-24T11:42:53.487625Z",
     "iopub.status.idle": "2024-03-24T11:42:53.501024Z",
     "shell.execute_reply": "2024-03-24T11:42:53.499635Z",
     "shell.execute_reply.started": "2024-03-24T11:42:53.488275Z"
    }
   },
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "#모델을 4bit 형식으로 로드(메모리 소비가 줄어듦)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0043a8bc-dd86-4900-b663-0a68a3a6c9ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:42:56.401261Z",
     "iopub.status.busy": "2024-03-24T11:42:56.400349Z",
     "iopub.status.idle": "2024-03-24T11:43:25.654006Z",
     "shell.execute_reply": "2024-03-24T11:43:25.653195Z",
     "shell.execute_reply.started": "2024-03-24T11:42:56.401198Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3348bea761a24808b437713029fb4d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='microsoft/phi-2'\n",
    "device_map = {\"\": 0}\n",
    "#모델을 양자화하여 다운(or load)\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3300fb-d2d7-4e9b-9128-d516ac7b8375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:43:39.453459Z",
     "iopub.status.busy": "2024-03-24T11:43:39.452724Z",
     "iopub.status.idle": "2024-03-24T11:43:39.919213Z",
     "shell.execute_reply": "2024-03-24T11:43:39.917523Z",
     "shell.execute_reply.started": "2024-03-24T11:43:39.453381Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#hugging face의 transformers 라이브로리를 사용하여 자연어 처리 모델에 대한 토크나이저를 설정하는 함수\n",
    "#AutoTokenizer.from_pretrained: 사전 훈련된 토크나이저를 불러옵니다.\n",
    "#model_name: 사용할 사전 훈련된 모델의 이름 또는 경로를 지정합니다.\n",
    "#trust_remote_code=True: 원격 코드를 신뢰하도록 지정합니다. 이것은 Hugging Face 모델 허브에서 토크나이저를 다운로드할 때 사용됩니다.\n",
    "#padding_side=\"left\": 패딩을 왼쪽에 추가합니다. 이는 토크나이저가 입력 시퀀스를 패딩할 때 어느 쪽에 패딩을 추가할지를 지정합니다.\n",
    "#add_eos_token=True: End-Of-Sequence (EOS) 토큰을 추가합니다. 이는 문장의 끝을 나타내는 특수 토큰입니다.\n",
    "#add_bos_token=True: Begin-Of-Sequence (BOS) 토큰을 추가합니다. 이는 문장의 시작을 나타내는 특수 토큰입니다.\n",
    "#use_fast=False: 빠른 토크나이저를 사용하지 않도록 설정합니다. 이것은 더 빠른 토크나이저가 아닌 표준 토크나이저를 사용하도록 강제합니다.\n",
    "tokenizer_ko = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-5.8b\",trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "#tokenizer.pad_token = tokenizer.eos_token: 이 부분은 패딩 토큰을 EOS (End-Of-Sequence) 토큰으로 설정합니다. 이렇게 하면 모델이 패딩을 식별하는 데 사용되는 토큰을 EOS 토큰으로 사용하게 됩니다.\n",
    "tokenizer_ko.pad_token = tokenizer_ko.eos_token\n",
    "\n",
    "#이 코드는 주어진 모델의 토크나이저를 설정하고, EOS 및 BOS 토큰을 추가하며, 패딩을 왼쪽에 추가하도록 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3418535d-0f1d-42e8-9f5e-2febf973820b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:43:43.850996Z",
     "iopub.status.busy": "2024-03-24T11:43:43.850062Z",
     "iopub.status.idle": "2024-03-24T11:43:44.474894Z",
     "shell.execute_reply": "2024-03-24T11:43:44.473174Z",
     "shell.execute_reply.started": "2024-03-24T11:43:43.850930Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#hugging face의 transformers 라이브로리를 사용하여 자연어 처리 모델에 대한 토크나이저를 설정하는 함수\n",
    "#AutoTokenizer.from_pretrained: 사전 훈련된 토크나이저를 불러옵니다.\n",
    "#model_name: 사용할 사전 훈련된 모델의 이름 또는 경로를 지정합니다.\n",
    "#trust_remote_code=True: 원격 코드를 신뢰하도록 지정합니다. 이것은 Hugging Face 모델 허브에서 토크나이저를 다운로드할 때 사용됩니다.\n",
    "#padding_side=\"left\": 패딩을 왼쪽에 추가합니다. 이는 토크나이저가 입력 시퀀스를 패딩할 때 어느 쪽에 패딩을 추가할지를 지정합니다.\n",
    "#add_eos_token=True: End-Of-Sequence (EOS) 토큰을 추가합니다. 이는 문장의 끝을 나타내는 특수 토큰입니다.\n",
    "#add_bos_token=True: Begin-Of-Sequence (BOS) 토큰을 추가합니다. 이는 문장의 시작을 나타내는 특수 토큰입니다.\n",
    "#use_fast=False: 빠른 토크나이저를 사용하지 않도록 설정합니다. 이것은 더 빠른 토크나이저가 아닌 표준 토크나이저를 사용하도록 강제합니다.\n",
    "tokenizer_phi_2 = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "#tokenizer.pad_token = tokenizer.eos_token: 이 부분은 패딩 토큰을 EOS (End-Of-Sequence) 토큰으로 설정합니다. 이렇게 하면 모델이 패딩을 식별하는 데 사용되는 토큰을 EOS 토큰으로 사용하게 됩니다.\n",
    "tokenizer_phi_2.pad_token = tokenizer_phi_2.eos_token\n",
    "\n",
    "#이 코드는 주어진 모델의 토크나이저를 설정하고, EOS 및 BOS 토큰을 추가하며, 패딩을 왼쪽에 추가하도록 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b41614-5090-4e96-bdef-e9a36f4fa5aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:43:46.622636Z",
     "iopub.status.busy": "2024-03-24T11:43:46.621727Z",
     "iopub.status.idle": "2024-03-24T11:43:46.637695Z",
     "shell.execute_reply": "2024-03-24T11:43:46.636375Z",
     "shell.execute_reply.started": "2024-03-24T11:43:46.622569Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_ko(model_name, prompt, max_length):\n",
    "    # 입력 텍스트를 토크나이즈하고 모델에 입력할 형식으로 변환\n",
    "    input_ids = tokenizer_ko.encode(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    # 모델에 입력하여 텍스트 생성\n",
    "    outputs = model_name.generate(input_ids, max_length=max_length)\n",
    "\n",
    "    # print(len(outputs))\n",
    "    # print(outputs)\n",
    "    \n",
    "    \n",
    "    # 생성된 텍스트 디코딩\n",
    "    generated_text = tokenizer_ko.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def gen_phi_2(model_name, prompt, max_length):\n",
    "    # 입력 텍스트를 토크나이즈하고 모델에 입력할 형식으로 변환\n",
    "    input_ids = tokenizer_phi_2.encode(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    # 모델에 입력하여 텍스트 생성\n",
    "    outputs = model_name.generate(input_ids, max_length=max_length)\n",
    "\n",
    "    # print(len(outputs))\n",
    "    # print(outputs)\n",
    "    \n",
    "    \n",
    "    # 생성된 텍스트 디코딩\n",
    "    generated_text = tokenizer_phi_2.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e47c2c6-6dbe-4bbd-a837-0d2d6c67d1fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:43:51.640261Z",
     "iopub.status.busy": "2024-03-24T11:43:51.639337Z",
     "iopub.status.idle": "2024-03-24T11:44:02.487135Z",
     "shell.execute_reply": "2024-03-24T11:44:02.485558Z",
     "shell.execute_reply.started": "2024-03-24T11:43:51.640196Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/usr/gatoai/python/venv/3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "왜 초밥에는 와사비가 들어갈까요? \n",
      "초밥은 안매운 음식인데, 와사비 때문에 너무 매울 때가 있습니다. 그래서 제가 궁금한 건 초밥에 와사비가 들어가는 이유가 무엇인가요?\n",
      "Output:\n",
      "왜 초밥에는 와사비가 들어가는 중가가*\u0006\u0006=6\u0006\u0006 키�약�하 약 일�딩 � 9 대� 국방 자본�� 커지��� 늦MC 자본*\u0006가운데�\u0006\u0006 들어간질 �E 스�총브라 뜻*�강�산 피( � ��산(�산� 어뵜\u0006\u0006질 사#X^W=)V 수성])vX^#\u0006\u0006 낫 28질�* 장영 정$(�%\u0006\u0006버린$ 낫 28%\u0006\u0006고마6\u0006대부E 접근 들 분리 접근 들이 접근 들 장비 긋\u0006\u0006=6\u0006\u0006 키�약�* 장영 정 전북 대� 내렸6\u0006가운데�\u0006\u0006 들어간질 �E 스�총브라 뜻*�강�산 피( � ��산(�\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruction:\n",
      "왜 초밥에는 와사비가 들어갈까요? \n",
      "초밥은 안매운 음식인데, 와사비 때문에 너무 매울 때가 있습니다. 그래서 제가 궁금한 건 초밥에 와사비가 들어가는 이유가 무엇인가요?\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "와사비는 기생충을 퇴치하기 위해 사용됩니다. \n",
      "초밥에 와사비를 함께 제공하는 이유는 그것이 기생충을 퇴치하기 위함입니다. 대부분의 날생선에는 기생충이 내장에 숨어있기 때문에, 그것을 제거하기 위해 고추냉이의 살균력을 활용하는 것입니다. 또한, 생강에도 살균 작용이 있어 식중독을 예방하는 역할을 합니다. 초밥집에서 얇게 쓴 생강을 제공하거나, 전갱이나 가다랭이 등에 생강을 곁들여 먹게 되는 것도 이와 같은 이유 때문입니다. 따라서, 와사비가 초밥과 함께 제공되는 것은 단순한 맛을 위한 것 뿐만 아니라, 위생적인 목적도 있기 때문입니다.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "왜 초밥에는 와사비가 들어가는 중가가*\u0006\u0006=6\u0006\u0006 키�약�하 약 일�딩 � 9 대� 국방 자본�� 커지��� 늦MC 자본*\u0006가운데�\u0006\u0006 들어간질 �E 스�총브라 뜻*�강�산 피( � ��산(�산� 어뵜\u0006\u0006질 사#X^W=)V 수성])vX^#\u0006\u0006 낫 28질�* 장영 정$(�%\u0006\u0006버린$ 낫 28%\u0006\u0006고마6\u0006대부E 접근 들 분리 접근 들이 접근 들 장비 긋\u0006\u0006=6\u0006\u0006 키�약�* 장영 정 전북 대� 내렸6\u0006가운데�\u0006\u0006 들어간질 �E 스�총브라 뜻*�강�산 피( � ��산(�\n",
      "CPU times: user 10.1 s, sys: 443 ms, total: 10.5 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 실행 시간을 알려주는 주피터 매직코드\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "## test를 위한 데이터\n",
    "prompt = dataset['train'][index]['instruction']\n",
    "output_data = dataset['train'][index]['output']\n",
    "\n",
    "# prompt 포맷\n",
    "formatted_prompt = f\"Instruction:\\n{prompt}\\nOutput:\\n\"\n",
    "\n",
    "# model 돌리기\n",
    "res = gen_ko(original_model, formatted_prompt, 250)\n",
    "\n",
    "# print('test 출력')\n",
    "# 결과 출력\n",
    "print(res)\n",
    "# output만 출력\n",
    "output = res.split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "# 입력 promt 출력\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "# 사람이 요약한 내용 출력\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{output_data}\\n')\n",
    "print(dash_line)\n",
    "# 모델이 요약한 내용 출력\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n",
    "\n",
    "## 위 모델은 요약은 하지 않지만 필수 정보들은 모두 추출하는 것을 보여주고 있음\n",
    "## 모델 fine-tuning 가능성 제시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91c2eb1f-6222-49aa-96ff-64b8475a9e1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:44:08.340813Z",
     "iopub.status.busy": "2024-03-24T11:44:08.340086Z",
     "iopub.status.idle": "2024-03-24T11:44:10.412270Z",
     "shell.execute_reply": "2024-03-24T11:44:10.411098Z",
     "shell.execute_reply.started": "2024-03-24T11:44:08.340750Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction:\n",
      "왜 초밥에는 와사비가 들어갈까요? \n",
      "초밥은 안매운 음식인데, 와사비 때문에 너무 매울 때가 있습니다. 그래서 제가 궁금한 건 초밥에 와사비가 들어가는 이유가 무엇인가요?\n",
      "Output:\n",
      "와사비가 들어가는 이유가 무�\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruction:\n",
      "왜 초밥에는 와사비가 들어갈까요? \n",
      "초밥은 안매운 음식인데, 와사비 때문에 너무 매울 때가 있습니다. 그래서 제가 궁금한 건 초밥에 와사비가 들어가는 이유가 무엇인가요?\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "와사비는 기생충을 퇴치하기 위해 사용됩니다. \n",
      "초밥에 와사비를 함께 제공하는 이유는 그것이 기생충을 퇴치하기 위함입니다. 대부분의 날생선에는 기생충이 내장에 숨어있기 때문에, 그것을 제거하기 위해 고추냉이의 살균력을 활용하는 것입니다. 또한, 생강에도 살균 작용이 있어 식중독을 예방하는 역할을 합니다. 초밥집에서 얇게 쓴 생강을 제공하거나, 전갱이나 가다랭이 등에 생강을 곁들여 먹게 되는 것도 이와 같은 이유 때문입니다. 따라서, 와사비가 초밥과 함께 제공되는 것은 단순한 맛을 위한 것 뿐만 아니라, 위생적인 목적도 있기 때문입니다.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "와사비가 들어가는 이유가 무�\n",
      "CPU times: user 1.97 s, sys: 67.5 ms, total: 2.03 s\n",
      "Wall time: 2.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 실행 시간을 알려주는 주피터 매직코드\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "## test를 위한 데이터\n",
    "prompt = dataset['train'][index]['instruction']\n",
    "output_data = dataset['train'][index]['output']\n",
    "\n",
    "# prompt 포맷\n",
    "formatted_prompt = f\"Instruction:\\n{prompt}\\nOutput:\\n\"\n",
    "\n",
    "# model 돌리기\n",
    "res = gen_phi_2(original_model, formatted_prompt, 250)\n",
    "\n",
    "# print('test 출력')\n",
    "# 결과 출력\n",
    "print(res)\n",
    "# output만 출력\n",
    "output = res.split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "# 입력 promt 출력\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "# 사람이 요약한 내용 출력\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{output_data}\\n')\n",
    "print(dash_line)\n",
    "# 모델이 요약한 내용 출력\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n",
    "\n",
    "## 위 모델은 요약은 하지 않지만 필수 정보들은 모두 추출하는 것을 보여주고 있음\n",
    "## 모델 fine-tuning 가능성 제시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f1d2f47-127b-4bdc-90ff-ebc62a8cae62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:44:15.197289Z",
     "iopub.status.busy": "2024-03-24T11:44:15.196572Z",
     "iopub.status.idle": "2024-03-24T11:44:15.216197Z",
     "shell.execute_reply": "2024-03-24T11:44:15.214749Z",
     "shell.execute_reply.started": "2024-03-24T11:44:15.197226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': '양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?', 'output': '양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.', 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=55320268'}\n",
      "---------------------------\n",
      "{'instruction': '양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?', 'output': '양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.', 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=55320268', 'text': '\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruct: Answer the questions below\\n\\n양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?\\n\\n### Output:\\n양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.\\n\\n### End'}\n",
      "---------------------------\n",
      "양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?\n",
      "---------------------------\n",
      "\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruct: Answer the questions below\n",
      "\n",
      "양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?\n",
      "\n",
      "### Output:\n",
      "양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \n",
      "\n",
      "식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\n",
      "\n",
      " 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \n",
      "\n",
      "고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.\n",
      "\n",
      "### End\n"
     ]
    }
   ],
   "source": [
    "# 위에서 다운 받은 데이터는 fine-tuning에 직접 사용될 수 없다\n",
    "# 모델이 이해할 수 있는 방식으로 프롬프트의 형식을 지정해서 전처리를 해야된다\n",
    "# HuggingFace 모델 문서를 참조하면 아래 지정된 형식의 대화 및 요약을 사용하여 프롬프트를 생성해야 한다\n",
    "# 대화-요약(프롬프트-응답) 쌍을 LLM에 대한 명시적 지침으로 변환해야 한다\n",
    "# text라는 애를 만듦\n",
    "# 입력을 프롬프트 형식으로 변환함\n",
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruct: Answer the questions below\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['instruction']}\" if sample[\"instruction\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample\n",
    "\n",
    "print(dataset['train'][0])\n",
    "print('---------------------------')\n",
    "print(create_prompt_formats(dataset['train'][0]))\n",
    "print('---------------------------')\n",
    "print(create_prompt_formats(dataset['train'][0])['instruction'])\n",
    "print('---------------------------')\n",
    "print(create_prompt_formats(dataset['train'][0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f600b72c-444e-4885-a42c-bc85ccdd1095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:44:20.220886Z",
     "iopub.status.busy": "2024-03-24T11:44:20.219516Z",
     "iopub.status.idle": "2024-03-24T11:44:20.241695Z",
     "shell.execute_reply": "2024-03-24T11:44:20.239896Z",
     "shell.execute_reply.started": "2024-03-24T11:44:20.220817Z"
    }
   },
   "outputs": [],
   "source": [
    "## prompt를 모델 토크나이저를 사용하여 토큰화된 프롬프트로 처리\n",
    "## 여기서의 목표는 일관된 길이의 입력 시퀀스를 생성하는 것\n",
    "## 일관된 길이의 입력 시퀀스는 효율성을 최적화하고 계산 오버헤드를 최소화하여 언어 모델을 미세 조정하는 데 도움이 된다.\n",
    "## 이러한 시퀀스가 모델의 최대 토큰 제한을 초과하지 않도록 확인하는 것이 중요하다\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "# 모델의 구성을 통해 최대 길이 설정을 가져오는 함수\n",
    "# 모델의 구성 중에서 \"n_positions\", \"max_position_embeddings\", \"seq_length\"와 같은 설정을 확인하여 최대 길이를 찾습니다.\n",
    "# 만약 최대 길이 설정이 없는 경우 기본값으로 1024를 사용합니다.\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "# \"n_positions\":\n",
    "    #이 설정은 모델이 처리할 수 있는 최대 위치 임베딩의 개수를 나타냅니다.\n",
    "    #위치 임베딩은 입력 토큰의 위치 정보를 인코딩하는 데 사용됩니다.\n",
    "    #따라서 이 값은 모델이 처리할 수 있는 최대 입력 시퀀스의 길이를 제한하는 데 사용됩니다.\n",
    "# \"max_position_embeddings\":\n",
    "    #이 설정은 모델이 처리할 수 있는 최대 입력 시퀀스의 길이를 나타냅니다.\n",
    "    #즉, 입력 시퀀스의 최대 길이가 이 값보다 작거나 같아야 합니다.\n",
    "    #이 값은 모델이 학습될 때 정의되며, 모델의 아키텍처에 따라 다를 수 있습니다.\n",
    "#\"seq_length\":\n",
    "    #이 설정은 모델이 입력으로 처리할 수 있는 최대 시퀀스의 길이를 나타냅니다.\n",
    "    #입력 시퀀스의 최대 길이가 이 값보다 작거나 같아야 합니다.\n",
    "    #이 값은 모델의 아키텍처에 따라 다르며, 주로 특정 언어 모델링 작업에 적합한 값을 선택하여 사용합니다.\n",
    "#이러한 설정들은 모델이 처리할 수 있는 입력의 형태와 길이를 제한하고, 모델의 효율성을 최적화하는 데 중요한 역할을 합니다.\n",
    "#이 값들은 모델을 초기화할 때 설정되며, 모델을 학습하는 동안 변경되지 않는다\n",
    "\n",
    "\n",
    "# 주어진 배치를 토크나이징하여 전처리하는 함수입니다.\n",
    "# 각 텍스트를 토크나이즈하고 최대 길이에 맞게 잘라내는 작업을 수행합니다.\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "#데이터셋을 전처리하여 모델 학습에 사용할 수 있는 형식으로 준비하는 함수입니다.\n",
    "    #먼저 각 샘플에 프롬프트를 추가합니다.\n",
    "    #다음으로 preprocess_batch 함수를 적용하여 각 배치를 전처리합니다. 이때 remove_columns 매개변수를 통해 불필요한 열을 제거합니다.\n",
    "    #입력 시퀀스의 길이가 최대 길이를 초과하는 샘플을 제거합니다.\n",
    "    #마지막으로 데이터셋을 섞습니다.\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    print(type(dataset))\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
    "    # partial: 함수를 편하게 만듦\n",
    "    # 여기서 데이터셋을 토큰화함\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    # 이러면 dataset에 text만 남음\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['url'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    # 입력 시퀀스의 길이가 max_length보다 큰 샘플을 필터링하여 제거\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f577682e-72e1-49e4-8972-b42d0e5dcb2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:44:23.326408Z",
     "iopub.status.busy": "2024-03-24T11:44:23.325412Z",
     "iopub.status.idle": "2024-03-24T11:44:42.298397Z",
     "shell.execute_reply": "2024-03-24T11:44:42.295932Z",
     "shell.execute_reply.started": "2024-03-24T11:44:23.326345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 2048\n",
      "2048\n",
      "{'instruction': '양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?', 'output': '양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \\n\\n식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\\n\\n 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \\n\\n고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.', 'url': 'https://kin.naver.com/qna/detail.naver?d1id=11&dirId=1116&docId=55320268'}\n",
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ceb583c5ff247f5a01688311a5c6f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55b068863e143749d03b27b5cc936c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe6f24c0d174d2cb40c748553af117a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/21155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': '케첩과 마요네즈의 입구 모양은 왜 다른가요? \\n보통 케찹과 마요네즈는 한 세트자나요? 그런데 보면 케찹은 원터치 뚜껑에다가 새 케찹은 뚜껑을 돌려서 열면 얇은 알미늄 같은 막도 이꾸또 케찹이 나오는 구멍이 동그랗자나요? 근데 마요네즈는 돌리는 뚜껑에다가 처음에 새로 사도 알미늄 같은 막두 엄꾸나오는 구멍 모양도 별 모양인데 왜 그런가요?', 'output': '케첩과 마요네즈의 입구 모양이 다른 것은 끈적이는 정도 즉, 물질의 점성(Viscosity) 차이 때문입니다. 마요네즈는 케첩보다 점성이 높기 때문에 돌리는 뚜껑에 별 모양의 입구를 만들어도 내부 액체의 모양이 유지됩니다. 하지만 케첩은 끈적거림이 적기 때문에 별 모양의 입구를 만들어도 케첩 내부의 액체가 곧 둥글게 뭉쳐지기 때문에 케첩의 뚜껑은 동그란 구멍입니다. 따라서 입구 모양은 물질의 점성(Viscosity) 차이 때문입니다.', 'text': '\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruct: Answer the questions below\\n\\n케첩과 마요네즈의 입구 모양은 왜 다른가요? \\n보통 케찹과 마요네즈는 한 세트자나요? 그런데 보면 케찹은 원터치 뚜껑에다가 새 케찹은 뚜껑을 돌려서 열면 얇은 알미늄 같은 막도 이꾸또 케찹이 나오는 구멍이 동그랗자나요? 근데 마요네즈는 돌리는 뚜껑에다가 처음에 새로 사도 알미늄 같은 막두 엄꾸나오는 구멍 모양도 별 모양인데 왜 그런가요?\\n\\n### Output:\\n케첩과 마요네즈의 입구 모양이 다른 것은 끈적이는 정도 즉, 물질의 점성(Viscosity) 차이 때문입니다. 마요네즈는 케첩보다 점성이 높기 때문에 돌리는 뚜껑에 별 모양의 입구를 만들어도 내부 액체의 모양이 유지됩니다. 하지만 케첩은 끈적거림이 적기 때문에 별 모양의 입구를 만들어도 케첩 내부의 액체가 곧 둥글게 뭉쳐지기 때문에 케첩의 뚜껑은 동그란 구멍입니다. 따라서 입구 모양은 물질의 점성(Viscosity) 차이 때문입니다.\\n\\n### End', 'input_ids': [202, 37, 6063, 9932, 21410, 224, 2818, 10903, 3864, 23184, 29828, 22253, 3432, 9059, 5862, 70, 13274, 69, 5862, 12063, 3628, 7245, 78, 17, 3792, 85, 25317, 12063, 21924, 19033, 2600, 426, 22253, 3432, 224, 24862, 6193, 83, 13274, 3432, 6063, 92, 224, 5967, 83, 4454, 87, 5862, 8945, 21924, 19928, 11273, 17, 202, 202, 6, 6, 6, 10276, 3864, 23184, 8701, 29, 1300, 81, 86, 90, 2423, 8945, 224, 19928, 11273, 4754, 86, 8517, 6063, 9932, 202, 202, 1293, 4742, 359, 29223, 978, 1110, 285, 6269, 2920, 296, 1511, 1108, 5674, 34, 224, 202, 13401, 4042, 442, 121, 359, 29223, 978, 1110, 272, 394, 5477, 341, 4389, 34, 1792, 446, 378, 4042, 442, 121, 296, 675, 17757, 13442, 10623, 858, 4042, 442, 121, 296, 13442, 276, 3836, 306, 785, 378, 8771, 296, 768, 603, 7251, 734, 296, 1247, 309, 307, 1953, 5032, 4042, 442, 121, 270, 1533, 272, 7809, 270, 25831, 341, 4389, 34, 2417, 29223, 978, 1110, 272, 5546, 272, 13442, 10623, 1388, 274, 3511, 20140, 768, 603, 7251, 734, 296, 1247, 752, 1489, 1953, 7627, 272, 7809, 2920, 309, 1611, 2920, 1767, 1511, 1006, 5674, 34, 202, 202, 6, 6, 6, 3060, 8846, 83, 8846, 29, 202, 1293, 4742, 359, 29223, 978, 1110, 285, 6269, 2920, 270, 1108, 388, 296, 28479, 270, 272, 1136, 2295, 15, 5407, 285, 906, 417, 11, 57, 5941, 3855, 86, 9705, 12, 2539, 818, 1441, 17, 29223, 978, 1110, 272, 4042, 4742, 886, 906, 12304, 878, 316, 818, 274, 5546, 272, 13442, 274, 1611, 2920, 285, 6269, 301, 1075, 2555, 2437, 15848, 285, 2920, 270, 1803, 10023, 17, 1146, 4042, 4742, 296, 28479, 17226, 270, 29451, 818, 274, 1611, 2920, 285, 6269, 301, 1075, 2555, 4042, 4742, 2437, 285, 15848, 293, 2360, 22173, 379, 16667, 9033, 818, 274, 4042, 4742, 285, 13442, 296, 18038, 822, 7809, 1441, 17, 3322, 6269, 2920, 296, 5407, 285, 906, 417, 11, 57, 5941, 3855, 86, 9705, 12, 2539, 818, 1441, 17, 202, 202, 6, 6, 6, 2229, 17130], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "print(dataset['train'][0])\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer_ko, max_length,seed, dataset)\n",
    "# eval_dataset = preprocess_dataset(tokenizer_ko, max_length,seed, dataset)\n",
    "\n",
    "print(train_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dcf29af-0c8c-44f1-8429-5109772ecdc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:44:48.512853Z",
     "iopub.status.busy": "2024-03-24T11:44:48.512093Z",
     "iopub.status.idle": "2024-03-24T11:44:48.580311Z",
     "shell.execute_reply": "2024-03-24T11:44:48.579396Z",
     "shell.execute_reply.started": "2024-03-24T11:44:48.512785Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "# Preparing the Model for QLoRA\n",
    "original_model = prepare_model_for_kbit_training(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5514878-1e3b-47c4-9360-c69e38526e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:44:49.978016Z",
     "iopub.status.busy": "2024-03-24T11:44:49.977148Z",
     "iopub.status.idle": "2024-03-24T11:44:50.377180Z",
     "shell.execute_reply": "2024-03-24T11:44:50.376446Z",
     "shell.execute_reply.started": "2024-03-24T11:44:49.977955Z"
    }
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc2f534d-9fff-44bb-b278-e2b49e96beab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:44:52.938229Z",
     "iopub.status.busy": "2024-03-24T11:44:52.937517Z",
     "iopub.status.idle": "2024-03-24T11:44:52.960416Z",
     "shell.execute_reply": "2024-03-24T11:44:52.958785Z",
     "shell.execute_reply.started": "2024-03-24T11:44:52.938166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trainable model parameters: 20971520\n",
      "all model parameters: 1542364160\n",
      "percentage of trainable model parameters: 1.36%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27e08c47-78cc-4ce4-845d-7bfa909dfe10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-24T11:44:55.051500Z",
     "iopub.status.busy": "2024-03-24T11:44:55.050783Z",
     "iopub.status.idle": "2024-03-24T11:44:55.089333Z",
     "shell.execute_reply": "2024-03-24T11:44:55.088273Z",
     "shell.execute_reply.started": "2024-03-24T11:44:55.051436Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/20223149/.local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./peft-ko-training-{str(int(time.time()))}'\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=25,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir = 'True',\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    eval_dataset=train_dataset['train'],\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer_ko, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2963b-5dfd-43de-832d-838062ca957e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T21:02:15.503607Z",
     "iopub.status.busy": "2024-03-17T21:02:15.502699Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 351/1000 6:51:34 < 12:45:21, 0.01 it/s, Epoch 0.07/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>8.780600</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>9.267200</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>8.832400</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>9.248000</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>8.692300</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>9.228900</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>8.730200</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>9.223400</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>8.772600</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>9.276800</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>8.692700</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>9.254500</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>8.810400</td>\n",
       "      <td>8.934643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='682' max='2645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 682/2645 07:55 < 22:51, 1.43 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/20223149/alpha_project/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f33566-220e-4c97-8417-109820dbb8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce10b5-ef4a-42fe-a5e7-fd0d6d05222b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_research",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
